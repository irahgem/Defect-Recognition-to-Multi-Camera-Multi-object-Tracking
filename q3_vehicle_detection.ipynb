{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Harish/Videos/Captures/samples')\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in c:\\users\\harish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.13.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  256\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [256 256   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           Vehicles\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                25\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'process_namespaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Harish\\Videos\\Untitled-1.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m             images_list\u001b[39m.\u001b[39mappend(image_label)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m images_list\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m train_images \u001b[39m=\u001b[39m transform_annotations(\u001b[39m'\u001b[39;49m\u001b[39mC:/Users/Harish/Downloads/archive/train/Final_Train_Dataset\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39mprint\u001b[39m(train_images[\u001b[39m0\u001b[39m:\u001b[39m5\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m dataset_train \u001b[39m=\u001b[39m DetectorDataset()\n",
      "\u001b[1;32mc:\\Users\\Harish\\Videos\\Untitled-1.ipynb Cell 3\u001b[0m in \u001b[0;36mtransform_annotations\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m height, width, _ \u001b[39m=\u001b[39m file_data\u001b[39m.\u001b[39mshape\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(image_path, split_img_path[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.xml\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mas\u001b[39;00m fd:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     \u001b[39m# Load the xml -> convert xml to dict -> convert to json\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     bb_file \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(json\u001b[39m.\u001b[39;49mdumps(xmltodict\u001b[39m.\u001b[39;49mparse(fd\u001b[39m.\u001b[39;49mread()), process_namespaces\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39m# There are two case - bb_file['annotation']['object'] can exist as a single dict or as a list of dict.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     \u001b[39m# Thus, we need to do a check to see whether it is a list or not.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m     \u001b[39m# If the value is a data type of list:\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(bb_file[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harish/Videos/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m         \u001b[39m# Loop through each dict in the list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harish\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[39m=\u001b[39mseparators, default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys,\n\u001b[0;32m    238\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39mencode(obj)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'process_namespaces'"
     ]
    }
   ],
   "source": [
    "class DetectorConfig(coco.CocoConfig):\n",
    "    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n",
    "    Overrides values in the base Config class.\n",
    "    \"\"\"\n",
    "    \n",
    "    NAME = 'Vehicles'\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "    \n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    NUM_CLASSES = 4  # background + 3 fruit classes\n",
    "    \n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    STEPS_PER_EPOCH = 25\n",
    "    \n",
    "config = DetectorConfig()\n",
    "config.display()\n",
    "\n",
    "class DetectorDataset(utils.Dataset):\n",
    "    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_labels(self, labels_list):\n",
    "        for i, label in enumerate(labels_list):\n",
    "            self.add_class('Vehicles', i + 1, label)\n",
    "            \n",
    "    def load_dataset(self, images_obj):\n",
    "        for image_obj in images_obj:\n",
    "            image_id = image_obj['image_id']\n",
    "            image_path = image_obj['image_path']\n",
    "            num_ids = image_obj['num_ids']\n",
    "            polygons = image_obj['polygons']\n",
    "            width = image_obj['width']\n",
    "            height = image_obj['height']\n",
    "            self.add_image(\"Vehicles\", image_id=image_id, path=image_path,\n",
    "                           width=width, height=height, polygons=polygons,num_ids=num_ids)\n",
    "    \n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        num_ids = info['num_ids']\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids\n",
    "    \n",
    "train_image_path = \"C:\\\\Users\\\\Harish\\\\Downloads\\\\archive\\\\train\"\n",
    "test_image_path = \"C:\\\\Users\\\\Harish\\\\Downloads\\\\archive\\\\test1\"\n",
    "\n",
    "labels = [\"car\", \"motorbike\", \"three wheelers (CNG)\", \"bus\", \"rickshaw\", \"pickup\", \"minivan\", \"van\", \"suv\", \"taxi\", \"truck\", \"bicycle\", \"policecar\", \"ambulance\",\n",
    "          \"human hauler\", \"wheelbarrow\", \"minibus\", \"auto rickshaw\", \"army vehicle\", \"scooter\"]\n",
    "\n",
    "\n",
    "def parse_single_annotation(label_obj):\n",
    "    \n",
    "    #print(label_obj)\n",
    "    name = label_obj['name']\n",
    "    # Get label\n",
    "    num_id = labels.index(name) + 1\n",
    "    bb_box = label_obj['bndbox']\n",
    "    # Extract the xmin xmax ymin and ymax of bounding box\n",
    "    xmin = int(bb_box['xmin'])\n",
    "    xmax = int(bb_box['xmax'])\n",
    "    ymin = int(bb_box['ymin'])\n",
    "    ymax = int(bb_box['ymax'])\n",
    "    # Convert it into polygon format. So we need 5 points for both x and y\n",
    "    all_points_x = [xmin, xmax, xmax, xmin, xmin]\n",
    "    all_points_y = [ymin, ymin, ymax, ymax, ymin]\n",
    "    return all_points_x, all_points_y, num_id\n",
    "\n",
    "import xmltodict\n",
    "import json\n",
    "train_images = []\n",
    "def transform_annotations(image_path):\n",
    "    # Start the index from 100\n",
    "    curr_idx = 100\n",
    "    images_list = []\n",
    "    # List the files in the training or test path\n",
    "    for i in os.listdir(os.path.join(image_path)):\n",
    "        # Get the image path\n",
    "        img_path = os.path.join(image_path, i)\n",
    "        split_img_path = i.split('.')\n",
    "        # check if the file is a .jpg ext. We ignore .xml file as they will be parsed based on .jpg file name\n",
    "        if split_img_path[1] == 'jpg':\n",
    "            # Define dict key value pair required in coco dataset\n",
    "            polygons = []\n",
    "            num_ids = []\n",
    "            # Read the image file \n",
    "            file_data = cv2.imread(img_path)\n",
    "            # Get the heigh and width. OpenCV shape is in the format h, w, depth\n",
    "            height, width, _ = file_data.shape\n",
    "            \n",
    "            with open(os.path.join(image_path, split_img_path[0] + '.xml')) as fd:\n",
    "                # Load the xml -> convert xml to dict -> convert to json\n",
    "                bb_file = json.loads(json.dumps(xmltodict.parse(fd.read()), process_namespaces=True))\n",
    "                # There are two case - bb_file['annotation']['object'] can exist as a single dict or as a list of dict.\n",
    "                # Thus, we need to do a check to see whether it is a list or not.\n",
    "                # If the value is a data type of list:\n",
    "                if isinstance(bb_file['annotation']['object'], list):\n",
    "                    # Loop through each dict in the list\n",
    "                    for obj in bb_file['annotation']['object']:\n",
    "                        # Parse each annotation individually\n",
    "                        all_points_x, all_points_y, num_id = parse_single_annotation(obj)\n",
    "                        # Append the points into polygon list\n",
    "                        polygons.append({\n",
    "                            'all_points_x': all_points_x,\n",
    "                            'all_points_y': all_points_y\n",
    "                        })\n",
    "                        # Append the id into the num_ids list\n",
    "                        num_ids.append(num_id)\n",
    "                # If the ['object'] key only contains a dict value\n",
    "                else:\n",
    "                    # We just need to parse a single annotation\n",
    "                    all_points_x, all_points_y, num_id = parse_single_annotation(bb_file['annotation']['object'])\n",
    "                    # Append it into polygon and num_ids list\n",
    "                    polygons.append({\n",
    "                        'all_points_x': all_points_x,\n",
    "                        'all_points_y': all_points_y\n",
    "                    })\n",
    "                    num_ids.append(num_id)\n",
    "            # For this image, we need to create a dict to represent it and all the corresponding annotations represented by polygons and num_ids key list\n",
    "            image_label = {\n",
    "                'image_path': img_path,\n",
    "                'image_id': curr_idx,\n",
    "                'polygons': polygons,\n",
    "                'num_ids': num_ids,\n",
    "                'height': height,\n",
    "                'width': width\n",
    "            }\n",
    "            curr_idx = curr_idx + 1\n",
    "            # Append it into the images_list\n",
    "            images_list.append(image_label)\n",
    "    return images_list\n",
    "\n",
    "train_images = transform_annotations('C:/Users/Harish/Downloads/archive/train/Final_Train_Dataset')\n",
    "print(train_images[0:5])\n",
    "dataset_train = DetectorDataset()\n",
    "dataset_train.load_labels(labels)\n",
    "dataset_train.load_dataset(train_images)\n",
    "dataset_train.prepare()\n",
    "\n",
    "test_images = transform_annotations(os.path.join(test_image_path, 'test'))\n",
    "dataset_val = DetectorDataset()\n",
    "dataset_val.load_labels(labels)\n",
    "dataset_val.load_dataset(test_images)\n",
    "dataset_val.prepare()\n",
    "\n",
    "from mrcnn import visualize\n",
    "import cv2\n",
    "print(len(train_images))\n",
    "for i in range(4):\n",
    "    \n",
    "    image_id = train_images[i]['image_id']\n",
    "    mask, num_id = dataset_train.load_mask(i)\n",
    "    img_data = cv2.imread(train_images[i]['image_path'])\n",
    "    num_id = [x - 1 for x in num_id]\n",
    "    visualize.display_top_masks(img_data, mask, num_id, labels)\n",
    "    \n",
    "sys.path.append(ROOT_DIR)\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'logs')\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "    \n",
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)\n",
    "    \n",
    "\n",
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=30, \n",
    "            layers='heads')\n",
    "\n",
    "\n",
    "\n",
    "os.listdir(MODEL_DIR)\n",
    "\n",
    "dir_names = os.listdir(MODEL_DIR)\n",
    "dir_names = sorted(dir_names)\n",
    "\n",
    "fps = []\n",
    "# Pick last directory\n",
    "for d in dir_names: \n",
    "    dir_name = os.path.join(MODEL_DIR, d)\n",
    "    # Find the last checkpoint\n",
    "    checkpoints = next(os.walk(dir_name))[2]\n",
    "    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n",
    "    checkpoints = sorted(checkpoints)\n",
    "    if not checkpoints:\n",
    "        print('No weight files in {}'.format(dir_name))\n",
    "    else: \n",
    "      \n",
    "      checkpoint = os.path.join(dir_name, checkpoints[-1])\n",
    "      fps.append(checkpoint)\n",
    "\n",
    "model_path = sorted(fps)[-1]\n",
    "print('Found model {}'.format(model_path))\n",
    "\n",
    "class InferenceConfig(DetectorConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode='inference', \n",
    "                          config=inference_config,\n",
    "                          model_dir=ROOT_DIR)\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)\n",
    "\n",
    "\n",
    "# set color for class\n",
    "def get_colors_for_class_ids(class_ids):\n",
    "    class_ids = [x - 1 for x in class_ids]\n",
    "    colors = []\n",
    "    for class_id in class_ids:\n",
    "        if class_id == 1:\n",
    "            colors.append((.941, .204, .204))\n",
    "    return colors\n",
    "\n",
    "\n",
    "# Show few example of ground truth vs. predictions on the validation dataset \n",
    "dataset = dataset_val\n",
    "fig = plt.figure(figsize=(10, 30))\n",
    "start_idx = 0\n",
    "for i in range(start_idx, start_idx + 3):\n",
    "    \n",
    "    image_id = random.choice(dataset.image_ids)\n",
    "    \n",
    "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config, \n",
    "                               image_id, use_mini_mask=False)\n",
    "    plt.subplot(6, 2, 2*(i-start_idx) + 1)\n",
    "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                                dataset.class_names,\n",
    "                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n",
    "    \n",
    "    plt.subplot(6, 2, 2*(i-start_idx) + 2)\n",
    "    results = model.detect([original_image]) #, verbose=1)\n",
    "    r = results[0]\n",
    "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset.class_names, r['scores'], \n",
    "                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d99ae87a09fc76145475e531a763d2e7bcff713a42f447201a2568c86df8c798"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
